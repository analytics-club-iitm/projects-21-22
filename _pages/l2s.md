---
layout: page
title: AI for Social Good - Listen to See
---

<h1>
    <center>AI for Social Good: Listen to See</center>
</h1>

# Description

An aid for visually impaired people to verbally visualize their surroundings. Given a frame/video sequence of a large scene, the AI must be able to explain the surroundings with sufficent context so that the person can move around. Beyond a social cause, it also helps dig deeper into open problems in Computer Vision - like capturing local context, temporal information across frames, handling different view points and occlusions. Further scope includes building a unified vision-language model which can also be utilized for various other downstream tasks. 

<p align="center">
    <img src="assets/images/l2s-1.png" width="100%">
    <img src="assets/images/l2s-2.png" width="100%">
</p>

# Application Process

To apply for this project one must attempt to solve a (simple)selection task. The task involves two parts: 
- Fill the missing portions of a starter code (each indicated with suitable comments and references to help easily find answers).
- Additional questions which are related to the project itself. There is no right or wrong answer to these questions and its basically to understand your thought process. 

<h3>
    <center><a style="background-color:#ffcc00;" href="assets/pdfs/l2s.pdf">Application Link</a></center>
</h3>

> **TIP**: Deep Learning was invented to mimic how a neuron/human learns (atleast to a certain degree). Keep this in mind while answering the questions above. 
Your answers need not be technically accurate (but it would be appreciated) and can be loosely worded, but they **MUST** have some intuition backing them.

# Submission Deadline

Completed applications are to be submitted by **7th May 2021 11:59PM**!

<h3>
    <center><a style="background-color:#ffcc00;" href="#">Submission Link (TO BE UPDATED)</a></center>
</h3>

<div class="container">
    <div class="countdown-styled" data-date="May 7, 2021 23:59:59"></div>
</div>